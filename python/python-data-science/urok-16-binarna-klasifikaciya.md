# Урок 16: Бінарна класифікація

**Вступ**

Класифікація набір параметрів в один із двох класів є типовим завданням машинного навчання. Можливо, ви плануєте передбачити, чи зможе клієнт здійснити покупку чи ні, чи була транзакція на кредитній картці шахрайською чи ні, чи сигнали глибокого космосу свідчать про нову планету чи ні, або медичні тести про захворювання є дійсними чи ні. Це все є проблеми бінарної класифікації.

У ваших вихідних даних класи можуть бути представлені такими рядками, як "`Так`" та "`Ні`", або "`Собака`" та "`Кішка`". Перш ніж використовувати ці дані, ми призначимо мітку класу: один клас відповідає `0` (по-людському: хиба), а інший - `1` (істина). Призначення числових міток приводить дані до форми, яку може опрацьовувати нейронна мережа.

**Точність та перехресна ентропія**

Точність є одним із багатьох показників, що використовуються для вимірювання успіху в класифікаційній задачі. Точність - це відношення правильних прогнозів до загальної кількості прогнозів:

> `точність = кількість_правильних / загальних.`

Наприклад, модель, яка завжди передбачає правильно, мала б бал точності `1,0`. За інших рівних умов точність є розумною метрикою для використання, коли класи в наборі даних відбуваються приблизно з однаковою частотою.

Проблема точності (та більшості інших метрик класифікації) полягає в тому, що її не можна використовувати як функцію втрат. SGD потребує функції збитків, яка змінюється плавно, але точність, будучи співвідношенням метрик, змінює "стрибки". Отже, ми повинні вибрати замінник, який діятиме як функція збитків. Цей замінник є функцією _`перехресної ентропії.`_

Тепер нагадаємо, що функція втрат визначає якість мережі під час навчання. Завдяки регресії, нашою метою було мінімізувати відстань між очікуваним результатом та прогнозованим результатом. Ми вибрали `MAE` для вимірювання цієї відстані.

Для класифікації, ми хочемо замість цього, відстань між ймовірностями, і саме це забезпечує перехресна ентропія. Перехресна ентропія - це свого роду міра для відстані від одного розподілу ймовірностей до іншого.

> Перехресна ентропія зменшує неправильні прогнози ймовірності.

Ідея полягає в тому, що ми хочемо, щоб наша мережа передбачала правильний клас з імовірністю`1.0`. Чим далі буде прогнозована ймовірність від `1,0`, тим більшими будуть перехресні ентропійні втрати.

Технічні причини, через які ми використовуємо перехресну ентропію, є делікатними, але основне, що слід зрозуміти з цього розділу, полягає лише в цьому: використовуйте перехресну ентропію для оцінки втрати класифікації; інші показники, які можуть вас турбувати (як точність), як правило, ідуть разом із ними.

**Створення ймовірностей за допомогою сигмовидної функції**

Для функцій перехресної ентропії та точності потрібні ймовірності як вхідні дані: тобто цифри від 0 до 1. Щоб перетворити реальні виходи, що створюються щільним шаром, у ймовірності, ми додаємо новий вид функції активації, сигмоїдної активації.

![image.png](https://firebasestorage.googleapis.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-MbjhfFkYBbYEPTQa-y0-887967055%2Fuploads%2Ffucf6stIZUb1c161JPqv%2Ffile.png?alt=media)

**ПРАКТИКА**

Для цього заняття ми будемо використовувати датасет **Ionosphere**.

Набір даних іоносфери містить характеристики, отримані від радіолокаційних сигналів, сфокусованих на іоносферному шарі атмосфери Землі.

Наше завдання полягає в тому, щоб визначити, чи сигнал показує наявність якогось об'єкта, чи просто повітря.

Посилання на датасет: [https://drive.google.com/file/d/1Cr7tapU\_GUVBPpLjcfoGLvX4zCSS8ED8/view?usp=sharing](https://drive.google.com/file/d/1Cr7tapU\_GUVBPpLjcfoGLvX4zCSS8ED8/view?usp=sharing)

```
import pandas as pd
from IPython.display import display

ion = pd.read_csv('../ion.csv', index_col=0) #пропишіть шлях
display(ion.head())

df = ion.copy()
df['Class'] = df['Class'].map({'good': 0, 'bad': 1})

df_train = df.sample(frac=0.7, random_state=0)
df_valid = df.drop(df_train.index)

max_ = df_train.max(axis=0)
min_ = df_train.min(axis=0)

df_train = (df_train - min_) / (max_ - min_)
df_valid = (df_valid - min_) / (max_ - min_)
df_train.dropna(axis=1, inplace=True) # видалити пусту колонку 2
df_valid.dropna(axis=1, inplace=True)

X_train = df_train.drop('Class', axis=1)
X_valid = df_valid.drop('Class', axis=1)
y_train = df_train['Class']
y_valid = df_valid['Class']
```

&#x20;Ми визначимо нашу модель так само, як це було зроблено для завдань з регресіями, за одним винятком. У кінцевому шарі включимо активацію `sigmoid`, щоб модель формувала ймовірності класу.

```
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(4, activation='relu', input_shape=[33]),
    layers.Dense(4, activation='relu'),    
    layers.Dense(1, activation='sigmoid'),
])
```

&#x20;Додайте метрику перехресних ентропій (`cross-entropy`) та точності до моделі за допомогою її методу компіляції. Для двокласних проблем обов’язково використовуйте бінарні опції. Оптимізатор `adam` чудово підходить і для класифікації, тому ми будемо використовувати його.

```
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['binary_accuracy'],
)
```

&#x20;Модель у цій конкретній задачі може зайняти чимало епох, щоб закінчити навчання, тому для зручності ми включимо `early_stopping`.

```
early_stopping = keras.callbacks.EarlyStopping(
    patience=10,
    min_delta=0.001,
    restore_best_weights=True,
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=1000,
    callbacks=[early_stopping],
    verbose=0, # hide the output because we have so many epochs
)
```

Розглянемо криві навчання, а також перевіримо найкращі значення втрат і точності, які ми отримали для набору перевірки:

```
history_df = pd.DataFrame(history.history)
# Start the plot at epoch 5
history_df.loc[5:, ['loss', 'val_loss']].plot()
history_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot()

print(("Best Validation Loss: {:0.4f}" +\
      "\nBest Validation Accuracy: {:0.4f}")\
      .format(history_df['val_loss'].min(), 
              history_df['val_binary_accuracy'].max()))
```
