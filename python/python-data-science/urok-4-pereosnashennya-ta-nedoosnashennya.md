# Урок 4: Переоснащення та недооснащення

епер, коли у вас є надійний спосіб вимірювання точності моделі, ви можете поекспериментувати з альтернативними моделями і подивитися, яка дає найкращі прогнози. Але які альтернативи у вас є для моделей?

Ви можете побачити в документації `scikit-learn`, що модель дерева рішень має безліч параметрів. Найважливіший з них -- це параметр, що визначає глибину дерева. Пригадайте з першого уроку в цьому курсі, що глибина дерева - це міра того, скільки сплітів воно робить перед тим, як дійти до прогнозу. Тоді це було відносно неглибоке дерево \(2 рівні\)

По мірі того, як дерево стає глибшим, набір даних ділиться на листя з меншою кількістю будинків. Якщо дерево мало лише 1 поділ, воно ділить дані на 2 групи. Якщо кожну групу розділити знову, ми отримаємо 4 групи будинків. Поділ кожного з них знову створить 8 груп. Якщо ми будемо продовжувати подвоювати кількість груп, додаючи більше поділів на кожному рівні, ми матимемо 2^10 груп будинків до того моменту, коли ми дійдемо до 10-го рівня. Це 1024 листки.

Коли ми ділимо будинки на багато листків, у нас також менше будинків на кожному з листків. Листя з дуже малою к-стю будинків даватимуть прогнози, які наближаються до фактичних значень в межах лиш ЦИХ будинків, але вони можуть робити дуже ненадійні прогнози щодо нових даних \(оскільки кожен прогноз базується лише на даних кількох будинків\).

Це явище називається **переоснащенням**. Модель майже ідеально справляється з перебаченнями в межах навчальних даних, але погано справляється з новими даними.

В крайньому випадку, якщо дерево ділить будинки лише на 2 або 4 рівні, то результатні прогнози можуть бути далекими для більшості будинків, навіть в межах навчальних даних. Коли модель не може вловити важливі відмінності та закономірності в даних, то вона погано працює навіть у навчальних даних, що називається **недооснащенням**.

Оскільки ми дбаємо про точність прогнозів для нових даних, то ми хочемо знайти найкращий баланс між недооснащенням та переоснащенням. Візуально нам потрібна нижня точка \(червоної\) кривої перевірки

![](../../.gitbook/assets/image%20%2886%29.png)

###  **Практика**

 _код з минулого заняття:_

```text
import pandas as pd

# посилання до датасету з даними про будинки Мельбурну
melbourne_file_path = '/content/drive/MyDrive/melb_data.csv'

# Створіть об'єкт, який міститиме в собі наш датасет
melbourne_data = pd.read_csv(melbourne_file_path) 

######################
melbourne_data.columns

# Дані будинків Мельбурну містять пусті комірки (деякі змінні деяких характеристик будинків не були записані)
# Ми навчимося як працювати з опущеними даними згодом 
# А поки, щоб уникнути труднощів, ми не враховуватимемо ВСІ будинки, у яких є пропущені змінні

melbourne_data = melbourne_data.dropna(axis=0)

y = melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']
X = melbourne_data[melbourne_features]

from sklearn.model_selection import train_test_split

# Тут ми розіб'ємо дані на навчальні та перевірочні: що для X та y
# Розбиття (далі: спліт) базується на основі генератора випадкових чисел.
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)
```

###  **Середня похибка в залежності від кількості сплітів**

Існує кілька варіантів керуванню глибини дерева, і багато з них дозволяють, щоб деякі маршрути через "дерева рішень" мали більшу глибину, ніж інші маршрути. Але аргумент `max_leaf_nodes` забезпечує дуже розумний спосіб контролю переоснащення та недооснащення. Чим більше листків ми дозволяємо зробити моделі, тим більше ми переходимо від зони недооснащення до зони переоснащення.

Ми можемо використовувати _функцію корисності_, щоб потім допомогти порівняти оцінки **MAE** з різними значеннями для `max_leaf_nodes`:

```text
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor

def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(train_X, train_y)
    preds_val = model.predict(val_X)
    mae = mean_absolute_error(val_y, preds_val)
    return(mae)
```

 Ми можемо використовувати цикл `for`, щоб порівняти точність моделей, побудованих з різними значеннями для `max_leaf_nodes`.

```text
# порівняй середню похибку при дедалі більших max_leaf_nodes
for max_leaf_nodes in [5, 50, 500, 5000]:
    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
    print("Max leaf nodes: %d  \t\t Середня похибка:  %d" %(max_leaf_nodes, my_mae))
```

```text
Max leaf nodes: 5  		   Середня похибка:  385696
Max leaf nodes: 50  		 Середня похибка:  279794
Max leaf nodes: 500  		 Середня похибка:  261718
Max leaf nodes: 5000  	 Середня похибка:  271996
```

 Звідси, робимо висновок, що найточніше модель працює в околицях значення "500" для `max leaf nodes`

