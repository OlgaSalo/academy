# Урок 12: Нейронні мережі

**Шари**

Нейронні мережі зазвичай організують свої нейрони в **шари**. Коли ми збираємо разом лінійні блоки, що мають загальний набір входів, ми отримуємо **щільний шар** (dense layer).

![image.png](https://firebasestorage.googleapis.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-MbjhfFkYBbYEPTQa-y0-887967055%2Fuploads%2FlWCKXiod4irducQhD1yb%2Ffile.png?alt=media)

Можна уявляти кожен шар нейронної мережі як якусь відносно просту трансформацію. Завдяки глибокому стеку шарів нейронна мережа може трансформувати свої входи все більш і більш складними способами. У добре натренованій нейронній мережі кожен шар є такою трансформацією, яка наближає нас якомога ближче до рішення.

**Функція активації**

Виявляється, що два щільних шари, без нічого проміжного, не кращі за один щільний шар сам по собі. Щільні шари самі по собі ніколи не зможуть вивестися зі світу лінійних функцій і площин. Нам потрібно щось _**нелінійне**_. Нам потрібні функції активації.

Функція активації - це просто якась функція, яку ми застосовуємо до кожного з вихідних даних шару (його активації). Найбільш поширеною є **випрямляча функція** `max (0, x)`.

![image.png](https://firebasestorage.googleapis.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-MbjhfFkYBbYEPTQa-y0-887967055%2Fuploads%2FRTp9CfcYxFPz2kiqYWzN%2Ffile.png?alt=media)

Функція випрямляча має графік, який є лінією з від'ємною частиною "випрямленою" до нуля.

Коли ми застосовуємо "випрямляч" до лінійного блоку, ми отримуємо випрямлений лінійний блок або **ReLU**. (З цієї причини прийнято називати випрямляючу функцію "функцією ReLU".) Застосування активації ReLU до лінійного блоку означає, що вихід стає

`max(0, w * x + b)`, що ми можемо накреслити на схемі, як :

![image.png](https://firebasestorage.googleapis.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-MbjhfFkYBbYEPTQa-y0-887967055%2Fuploads%2FlcFPc5hyOfk2xUGLtx0m%2Ffile.png?alt=media)

**Упорядковуючи щільні шари**

Тепер, коли у нас нарешті є певна нелінійність, давайте подивимося, як можна скласти шари, щоб отримати складні перетворення даних

![image.png](https://firebasestorage.googleapis.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-MbjhfFkYBbYEPTQa-y0-887967055%2Fuploads%2F9J0U7IjMblY5QNdNk7LA%2Ffile.png?alt=media)

Шари перед вихідним шаром іноді називають _**прихованими**_, оскільки ми ніколи не бачимо їх результати _безпосередньо_.

Тепер зауважте, що кінцевий (вихідний) шар є лінійним блоком (тобто з відсутньою функцією активації). Це робить цю мережу придатною для завдання регресії, де ми намагаємося передбачити якесь довільне числове значення. Інші завдання (наприклад _класифікація_) можуть вимагати активації функції на виході.

**ПРАКТИКА**

Модель `Sequential` , яку ми використовували, з’єднує список шарів у порядку від першого до останнього: перший шар отримує вхідні дані, останній шар видає вихідні дані. Це створює модель на малюнку вище:

```
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    # приховані ReLU шари
    layers.Dense(units=4, activation='relu', input_shape=[2]),
    layers.Dense(units=3, activation='relu'),
    # лінійний вихідний шар
    layers.Dense(units=1),
])
```
