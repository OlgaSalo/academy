# Урок 15: Batch нормалізація

На цьому уроці ми дізнаємося про два типи спеціальних шарів, які не містять власне нейронів, але додають певну функціональність, яка іноді може по-різному принести користь моделі. Обидва шари зазвичай використовуються в сучасних архітектурах.

**DROPOUT**

Перший з них - це "шар, що відпадає" \(**dropout**\), який може допомогти у виправленні переоснащення.

На останньому уроці ми говорили про те, як переоснащення спричинене оманливо правдоподібними закономірностями мережевого навчання при навчальних даних. Для розпізнавання цих помилкових закономірностей мережа часто покладається на дуже конкретні комбінації ваги, свого роду "змову" ваг. Будучи настільки конкретними, ці змови, як правило, крихкі: видаліть одну, і змова розвалюється.

Це ідея відмови від навчання. Щоб розірвати ці змови, ми випадковим чином викидаємо частину блоків на кожному етапі навчання, що ускладнює мережі вивчення цих помилкових зразків у навчальних даних. Натомість йому доводиться шукати більш загальні закономірності, чиї структури "ваг", як правило, є більш надійними.

Додавання **dropout**

У `Keras` коефіцієнт `rate` щодо droput визначає, який відсоток вхідних блоків потрібно відключити у момент часу. Помістіть шар "dropout" безпосередньо перед шаром, до якого потрібно застосувати dropout:

```text
keras.Sequential([
    # ...
    layers.Dropout(rate=0.3), # застосуйте 30% dropout до наступного шару
    layers.Dense(16),
    # ...
])
```

**BATCH нормалізація**

Наступний спеціальний шар, який ми розглянемо, виконує **batch нормалізацію** \(або "_batchnorm_"\), що може допомогти скорегувати повільне або нестабільне тренування.

Для нейронних мереж загалом є гарною ідеєю розмістити всі ваші дані в загальному масштабі, можливо, за допомогою чогось на кшталт `StandardScaler` або `MinMaxScaler` з бібліотеки `scikit-learn`. Причина полягає в тому, що стохастичний градієнт зміщуватиме ваги мережі пропорційно величині активації, яку виробляють дані. Особливості, які мають тенденцію активувати дуже різні розміри, можуть призвести до нестабільної поведінки в навчанні.

Тепер, якщо добре нормалізувати дані до того, як вони потраплять у мережу, можливо, нормалізація всередині мережі буде кращою!

Насправді, у нас є спеціальний тип шару, який може це зробити: batch нормалізація. Рівень нормалізації batch розглядає кожен batch в міру надходження, спочатку нормалізуючи партію із власним середнім значенням та стандартним відхиленням, а потім розміщуючи дані в новому масштабі з двома параметрами масштабування, що піддаються навчанню.

> Batchnorm, по суті, виконує своєрідне скоординоване масштабування своїх входів.

Найчастіше batchnorm додають як допоміжний процес для оптимізації \(хоча іноді він також може сприяти прогнозуванню ефективності\). Моделі з Batchnorm, як правило, потребують менше епох для завершення навчання. Більше того, batchnorm також може виправити різні проблеми, які можуть призвести до того, що навчання "застрягне". Подумайте про те, щоб додати до своїх моделей нормалізацію партії, особливо якщо у вас виникають проблеми під час навчання.

**Додавання batchnorm**

Batchnorm можна використовувати практично в будь-якій точці мережі. Ви можете покласти його після шару або між ними

```text
layers.Dense(16, activation='relu'),
layers.BatchNormalization(),
```

або

```text
layers.Dense(16),
layers.BatchNormalization(),
layers.Activation('relu'),
```

**ПРАКТИКА**

Продовжимо розробку моделі для датасету Red Wine. Тепер ми збільшимо об'єм моделі ще більше, але додамо dropout та batch нормалізацію, щоб прискорити оптимізацію. Цього разу ми також опустимо масштабування даних, щоб продемонструвати, як нормалізація партії може стабілізувати навчання.

```text
# Setup plotting
import matplotlib.pyplot as plt

plt.style.use('seaborn-whitegrid')
# Set Matplotlib defaults
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)


import pandas as pd

red_wine = pd.read_csv("/content/drive/MyDrive/winequality-red.csv") #тут вказуємо шлях до датасету на вашому диску

# Створіть навчальні та перевірочні набори даних
df_train = red_wine.sample(frac=0.7, random_state=0)
df_valid = red_wine.drop(df_train.index)

# Розділіть параметри та цілі прогнозування
X_train = df_train.drop('quality', axis=1)
X_valid = df_valid.drop('quality', axis=1)
y_train = df_train['quality']
y_valid = df_valid['quality']
```

Під час додавання dropout вам може знадобитися збільшити кількість блоків у ваших щільних шарах.

```text
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(1024, activation='relu', input_shape=[11]),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1),
])
```

Інші настройки -- без змін

```text
model.compile(
    optimizer='adam',
    loss='mae',
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=100,
    verbose=0,
)


# Show the learning curves
history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();
```

![](../../.gitbook/assets/image%20%2884%29.png)

Зазвичай ви отримуєте кращу продуктивність, якщо стандартизуєте свої дані перед тим, як використовувати їх для навчання. Однак те, що ми взагалі змогли використовувати необроблені дані, показує, наскільки ефективною може бути dropout та batch нормалізація на більш складних наборах даних.

