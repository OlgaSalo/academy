# Урок 14: Переоснащення і недооснащення (overfitting and underfitting) в контексті нейронних мереж

**Інтерпретуючи криві навчання**

Можна вважати, що інформація у навчальних даних є двох видів: **сигнальна** та **шумова**:

**Сигнал** - це частина, яка узагальнює; яка може допомогти нашій моделі робити точніші прогнози на основі нових даних. **Шум** - це всі випадкові, неінформативні закономірності, які можуть не допомогти моделі робити влучні прогнози. Шум - це частина, яка може здаватися корисною, але насправді ні.

Ми тренуємо модель, вибираючи ваги або параметри, які мінімізують втрати на тренувальному наборі. Ти, однак, можеш знати, що для точної оцінки ефективності моделі нам потрібно протестувати її на перевірочному наборі даних: **даних перевірки**.

Коли ми тренуємо модель, ми будували графік втрат на епоху за епохою на тренувальних даних (_training_). Тепер до цього ми також додамо графік даних перевірки (_validation_). Щоб ефективно тренувати моделі глибокого навчання, нам потрібно вміти їх інтерпретувати.

![image.png](https://firebasestorage.googleapis.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-MbjhfFkYBbYEPTQa-y0-887967055%2Fuploads%2F1BUcJcZLXfQEcdMAVvh9%2Ffile.png?alt=media)

Втрати для тренувань зменшуються **або** тоді, коли модель засвоїть сигнал, **або** коли вона вивчить шум. Але втрати під час _перевірки_ зменшаться лише тоді, коли модель **засвоїть** сигнал. (Який би шум не вивчала модель з навчального набору, це не допоможе будувати прогнози на нових даних.) Отже, коли модель засвоює сигнал, обидві криві падають, але коли вона фіксує здебільшого шум, між кривими створюється розрив. Розмір розриву говорить вам, скільки шуму вивчила модель.

В ідеалі ми хотіли б створити моделі, які вивчають весь сигнал і жоден шум. Цього практично ніколи не станеться. Натомість ми немов торгуємось з комп'ютером: поки торгівля йде на нашу користь, втрати від перевірки продовжуватимуть зменшуватися. Однак через певний момент торгівля може обернутися проти нас: втрати перевищують вигоду, і втрати від перевірки починають зростати.

Коли модель занадто охоче навчається, втрати під час перевірки можуть почати перебільшувати ті, ніж що під час навчання. Щоб запобігти цьому, ми можемо просто **припинити навчання**, коли здається, що втрата перевірки більше не зменшується. Переривання тренування таким чином називається ранньою зупинкою (**early stopping**).

**Потужність моделі**

Потужність моделі стосується її розміру та складності зразків, які вона здатна вивчити. Для нейронних мереж це значною мірою визначатиметься тим, скільки нейронів вона має і як вони з’єднані між собою. Якщо виявляється, що у вашій мережі недостатньо даних, спробуйте збільшити її пропускну здатність.

Ви можете збільшити потужність мережі або зробивши її ширшою (більше блоків до існуючих шарів), або зробивши її глибшою (додавши більше шарів). Ширші мережі легше засвоюють лінійні співвідношення, тоді як більш глибокі мережі надають перевагу більш нелінійним.

Що краще? Залежить лише від набору даних.

```
#типова модель
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(16, activation='relu'),
    layers.Dense(1),
])


#ширша модель
wider = keras.Sequential([
    layers.Dense(32, activation='relu'),
    layers.Dense(1),
])

#глибша модель
deeper = keras.Sequential([
    layers.Dense(16, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(1),
])
```

**ПРАКТИКА**

Додаємо ранню зупинку:

У `Keras` ми включаємо ранню зупинку в наші тренування через імпорт `callback`. `callback` - це просто функція, яку потрібно запускати щоразу, коли мережа тренується. Рання зупинка - це один із `callback`, який буде виконуватися після кожної епохи.

```
from tensorflow.keras.callbacks import EarlyStopping #імпортимо callback

early_stopping = EarlyStopping(
    min_delta=0.001, # мінімальна зміна, яка буде вважатися як покращення точності
    patience=20, # скільки епох чекати після того, як модель запідозрить переоснащення
    restore_best_weights=True,
)
```

Налаштовуємо датасет з минулого заняття:

```
import pandas as pd
from IPython.display import display

red_wine = pd.read_csv("/content/drive/MyDrive/winequality-red.csv") #тут вказуємо шлях до датасету на вашому диску

# ділимо дані на навчальні та перевірочні
df_train = red_wine.sample(frac=0.7, random_state=0)
df_valid = red_wine.drop(df_train.index)
display(df_train.head(4))

# Масштабуємо дo [0, 1]
max_ = df_train.max(axis=0)
min_ = df_train.min(axis=0)
df_train = (df_train - min_) / (max_ - min_)
df_valid = (df_valid - min_) / (max_ - min_)

# розділимо параметри та цілі прогнозування
X_train = df_train.drop('quality', axis=1)
X_valid = df_valid.drop('quality', axis=1)
y_train = df_train['quality']
y_valid = df_valid['quality']
```



| <p><br>fixed acidity</p> | volatile acidity | citric acid | residual sugar | chlorides | free sulfur dioxide | total sulfur dioxide | density | pH      | sulphates | alcohol | quality |   |
| ------------------------ | ---------------- | ----------- | -------------- | --------- | ------------------- | -------------------- | ------- | ------- | --------- | ------- | ------- | - |
| 1109                     | 10.8             | 0.470       | 0.43           | 2.10      | 0.171               | 27.0                 | 66.0    | 0.99820 | 3.17      | 0.76    | 10.8    | 6 |
| 1032                     | 8.1              | 0.820       | 0.00           | 4.10      | 0.095               | 5.0                  | 14.0    | 0.99854 | 3.36      | 0.53    | 9.6     | 5 |
| 1002                     | 9.1              | 0.290       | 0.33           | 2.05      | 0.063               | 13.0                 | 27.0    | 0.99516 | 3.26      | 0.84    | 11.7    | 7 |
| 487                      | 10.2             | 0.645       | 0.36           | 1.80      | 0.053               | 5.0                  | 14.0    | 0.99820 | 3.17      | 0.42    | 10.0    | 6 |

Тепер давайте збільшимо потужність (вона ж ємність) мережі.

Ми запустимо досить велику мережу, покладаючись на ранню зупинку, щоб припинити навчання, як тільки втрата перевірки покаже ознаки переоснащення.

```
model = keras.Sequential([
    layers.Dense(512, activation='relu', input_shape=[11]),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(1),
])
model.compile(
    optimizer='adam',
    loss='mae',
)
```

&#x20;Коли запускатимемо функцію `fit`, додамо ранню зупинку:

```
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=500, #задаємо максимальну кількість епох
    callbacks=[early_stopping], # ось тут додаємо ранню зупинку
    verbose=0,  
)

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();
print("Найменша втрата перевірки: {}".format(history_df['val_loss'].min()))
```

```
Найменша втрата перевірки: 0.09128808975219727
```

![](https://firebasestorage.googleapis.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-MbjhfFkYBbYEPTQa-y0-887967055%2Fuploads%2FidcZHhxFhSi29HfVPdAx%2Ffile.png?alt=media)

Видно, що модель перестала тренуватися далеко від заданих 500 епох через early stopping!
