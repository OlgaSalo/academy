# Урок 6: пропущені дані

Існує багато шляхів, коли набір даних може опинитися з відсутніми значеннями.

Наприклад,

1. Будинок з 2 спальнями не включає значення для розміру третьої спальні.
2. Респондент опитування може вирішити не ділитися своїми доходами.

Більшість бібліотек машинного навчання \(включаючи `scikit-learn`\) видають помилку при спробі побудувати модель із використанням даних із відсутніми значеннями. Тож вам потрібно буде використати одну із наведених нижче стратегій.

###  **Три стратегії**

1\) _Простий варіант_: видаліть всі стовпці з відсутніми значеннями.

Але якщо більшість значень у скинутих стовпцях відсутні, то модель втрачає доступ до великої кількості \(потенційно корисної!\) інформації за такого підходу. Як екстремальний але цілком можливий приклад, розглянемо набір даних із 10000 рядків, де в одному важливому стовпці бракує одного запису. Такий підхід повністю опустив би колонку!

2\) Кращий варіант: **імпутація**

Імпутація заповнює пропущені значення деяким числом. Наприклад, ми можемо заповнити пусті комірки середнім чи типовим значення вздовж кожного стовпця.

Приписане значення в більшості випадків буде не зовсім правильним, але воно, як правило, призводить до отримання більш точних моделей, ніж ви отримаєте від повного видалення стовпця.

3\) **Розширена імпутація** Імпутація - це стандартний підхід, який зазвичай працює добре. Однак обчислені значення можуть систематично перевищувати чи переменшувати їх фактичні значення \(які не були зібрані в наборі даних\). Або рядки з відсутніми значеннями можуть бути унікальними якимось іншим чином. У цьому випадку ваша модель може робити кращі прогнози, _враховуючи_, які значення спочатку відсутні.

У цьому підході ми імпутуємо відсутні значення, як і раніше. Але крім того, для кожного стовпця з відсутніми записами у вихідному наборі даних ми додаємо новий стовпець, який показує розташування імпутованих записів.

У деяких випадках це суттєво покращить результати. В інших випадках це зовсім не допомагає.

![](../../.gitbook/assets/image%20%2886%29.png)

###  **Практика**

```text
import pandas as pd
from sklearn.model_selection import train_test_split

# завантаж дані
melbourne_file_path = '/content/drive/MyDrive/melb_data.csv'
data = pd.read_csv(melbourne_file_path)

# Обери ціль для прогнозів
y = data.Price

# Для упрощення задачі, ми дропнемо всі нечислові стовпці
melb_predictors = data.drop(['Price'], axis=1)
X = melb_predictors.select_dtypes(exclude=['object'])

# Ділимо дані на навчальні та тренувальні підгрупи
X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,
                                                      random_state=0)
```

_Визначте функцію для вимірювання якості кожного підходу_

Визначаємо функцію `score_dataset()` для порівняння різних підходів до роботи з відсутніми значеннями. Ця функція повертає середню абсолютну похибку \(MAE\) із `random forest` моделі.

```text
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# Функція, яку ми використаємо для порівняння кожного з підходів
def score_dataset(X_train, X_valid, y_train, y_valid):
    model = RandomForestRegressor(n_estimators=10, random_state=0)
    model.fit(X_train, y_train)
    preds = model.predict(X_valid)
    return mean_absolute_error(y_valid, preds)
```

###  **Оцінка першого підходу**

Оскільки ми працюємо як з навчальними, так і з наборами перевірки, ми обережно видаляємо однакові стовпці в обидва `DataFrames`.

```text
# колонки з пропущеними даними
cols_with_missing = [col for col in X_train.columns
                     if X_train[col].isnull().any()]

# Видаляємо вищезазначені колонки
reduced_X_train = X_train.drop(cols_with_missing, axis=1)
reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)

print("MAE з Підходу 1 (видалити стовпці з пропущеними значеннями):")
print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))
```

```text
MAE з Підходу 1 (видалити стовпці з пропущеними значеннями): 183550.22137772635
```

###  **Оцінка другого підходу**

Далі ми використовуємо `SimpleImputer`, щоб замінити відсутні значення на середнє значення вздовж кожного стовпця.

Хоча це примітивний підхід, заповнення середнього значення, як правило, виконується досить добре \(але це залежить від набору даних\). Хоча статистики експериментували з більш складними способами визначення обчислюваних значень \(імпутація регресії, наприклад\), складні стратегії, як правило, не дають додаткової продуктивності.

```text
from sklearn.impute import SimpleImputer

# Імпутація
my_imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))

# Імпутація прибрала назви колонок; вертаємо їх назад!
imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns

print("MAE з підходу 2 (Імпутація):")
print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))
```

```text
MAE з підходу 2 (Імпутація):
178166.46269899711
```

Можна уже помітити, що МАЕ для другого підходу менший, ніж для першого підходу. Отже, другий підхід більш близький до моделювання реальності

###  **Оцінка третього підходу**

Далі ми імпутуємо відсутні значення, одночасно відстежуючи, які значення були введені.

```text
# Робимо копію даних
X_train_plus = X_train.copy()
X_valid_plus = X_valid.copy()

# Нові колонки, які будуть локалізувати імпутовані дані
for col in cols_with_missing:
    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()
    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()

# Власне імпутація
my_imputer = SimpleImputer()
imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))
imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))

# Iмпутація прибрала назви колонок; вертаємо їх назад!
imputed_X_train_plus.columns = X_train_plus.columns
imputed_X_valid_plus.columns = X_valid_plus.columns

print("MAE з підходу 3 (Розширена імпутація):")
print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))
```

```text
MAE з підходу 3 (Розширена імпутація):
178927.503183954
```

Як ми бачимо, Підхід 3 моделює трохи гірше, ніж Підхід 2.

**Отже, чому імпутація вийшла краще, ніж скидання стовпців?**

Навчальні дані містять 10864 рядки та 12 стовпців, де три стовпці містять відсутні дані. Для кожного стовпця відсутня менше половини записів. Таким чином, скидання стовпців видаляє багато корисної інформації, і тому має сенс, що імпутація буде ефективнішою.

